{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from supabase import create_client, Client\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "supabase_url = os.getenv(\"SUPABASE_URL\")\n",
    "supabase_key = os.getenv(\"SUPABASE_KEY\")\n",
    "\n",
    "print(\"Loading environment variables...\")\n",
    "supabase = create_client(supabase_url, supabase_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c87d7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterator, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "class DocumentLoaderHelper:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def get_urls_from_json(self) -> list[str]:\n",
    "        urls = []\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = load(file)\n",
    "            if not isinstance(data, list):\n",
    "                print(f\"Error: The '{self.filepath}' is not a JSON list.\")\n",
    "                return []\n",
    "\n",
    "            for source_group in data:\n",
    "                if source_group.get(\"active\") and isinstance(\n",
    "                    source_group.get(\"urls\"), list\n",
    "                ):\n",
    "                    for url in source_group[\"urls\"]:\n",
    "                        if isinstance(url, str) and url.strip().startswith(\"http\"):\n",
    "                            urls.append(url.strip())\n",
    "        return urls\n",
    "\n",
    "    def load_document(self):\n",
    "        file = open(self.file_path, \"r\")\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        return text\n",
    "\n",
    "\n",
    "class SplitMarkdownDocumentsLoader(BaseLoader):\n",
    "    def __init__(self, documents: List[Document]):\n",
    "        self.documents = documents\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        for doc in self.documents:\n",
    "            yield doc\n",
    "\n",
    "\n",
    "def extract_book_info(content, filepath):\n",
    "    \"\"\"Extract book title and author from content or filename\"\"\"\n",
    "    # Try to extract from content first\n",
    "    lines = content.split(\"\\n\")[:10]  # Check first 10 lines\n",
    "    title = author = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"# \") and not title:\n",
    "            title = line[2:].strip()\n",
    "        elif \"author:\" in line.lower():\n",
    "            author = line.split(\":\", 1)[1].strip()\n",
    "        elif \"by \" in line.lower() and not author:\n",
    "            author = line.lower().split(\"by \")[1].strip()\n",
    "\n",
    "    # Fallback to filename\n",
    "    if not title:\n",
    "        filename = Path(filepath).stem\n",
    "        title = filename.replace(\"_\", \" \").replace(\"-\", \" \").title()\n",
    "\n",
    "    if not author:\n",
    "        author = \"Unknown Author\"\n",
    "\n",
    "    return title, author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61319d75",
   "metadata": {},
   "source": [
    "Peter Atia site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666fbfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import bs4\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1300,\n",
    "    chunk_overlap=300,\n",
    "    separators=[\n",
    "        \"\\n\\n\",  # Paragraph breaks\n",
    "        \"\\n\",  # Line breaks\n",
    "        \". \",  # Sentence endings\n",
    "        \"! \",  # Exclamations\n",
    "        \"? \",  # Questions\n",
    "        \" \",  # Word boundaries\n",
    "        \"\",  # Character level (fallback)\n",
    "    ],\n",
    ")\n",
    "\n",
    "web_loader = WebBaseLoader(\n",
    "    web_path=DocumentLoaderHelper(\"config/web_sources.json\").get_urls_from_json(),\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(class_=(\"entry-content\")),\n",
    "    },\n",
    "    bs_get_text_kwargs={\"separator\": \" \", \"strip\": True},\n",
    ")\n",
    "\n",
    "web_documents = []\n",
    "async for document in web_loader.alazy_load():\n",
    "    chunks = text_splitter.split_documents([document])\n",
    "    web_documents.extend(chunks)\n",
    "\n",
    "print(f\"Loaded {len(web_documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c20c5",
   "metadata": {},
   "source": [
    "Book summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "import re\n",
    "\n",
    "# Define headers to split on\n",
    "headers_to_split_on = [(\"#\", \"Title\"), (\"##\", \"Section\"), (\"###\", \"Subsection\")]\n",
    "\n",
    "# Create the markdown splitter once - strip headers and clean content\n",
    "md_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    strip_headers=True,  # This removes the header markdown tags\n",
    ")\n",
    "\n",
    "book_documents = []\n",
    "book_files = glob.glob(\"downloads/books/*.md\")\n",
    "\n",
    "for filepath in book_files:\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Extract book metadata\n",
    "    title, author = extract_book_info(content, filepath)\n",
    "\n",
    "    # Split by markdown headers - pass content string, not Document object\n",
    "    chunks = md_splitter.split_text(content)\n",
    "\n",
    "    print(f\"\\nProcessing: {title} by {author}\")\n",
    "    print(f\"Found {len(chunks)} sections\")\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # chunk is already a Document object from MarkdownHeaderTextSplitter\n",
    "\n",
    "        # Clean up the content - remove extra newlines and whitespace\n",
    "        cleaned_content = chunk.page_content.strip()\n",
    "        # Replace multiple newlines with single newlines\n",
    "        cleaned_content = re.sub(r\"\\n{3,}\", \"\\n\\n\", cleaned_content)\n",
    "        # Remove leading/trailing whitespace from each line\n",
    "        cleaned_content = \"\\n\".join(\n",
    "            line.strip() for line in cleaned_content.split(\"\\n\")\n",
    "        )\n",
    "\n",
    "        # Update the chunk content\n",
    "        chunk.page_content = cleaned_content\n",
    "\n",
    "        # Get section info from the header metadata\n",
    "        section_title = \"Unknown Section\"\n",
    "        if hasattr(chunk, \"metadata\") and chunk.metadata:\n",
    "            # Extract section title from header metadata\n",
    "            if \"Section\" in chunk.metadata:\n",
    "                section_title = chunk.metadata[\"Section\"]\n",
    "            elif \"Title\" in chunk.metadata:\n",
    "                section_title = chunk.metadata[\"Title\"]\n",
    "\n",
    "        # Update metadata with book information\n",
    "        chunk.metadata.update(\n",
    "            {\n",
    "                \"source\": filepath,\n",
    "                \"source_type\": \"book_summary\",\n",
    "                \"book_title\": title,\n",
    "                \"author\": author,\n",
    "                \"section_title\": section_title,\n",
    "                \"chunk_index\": i,\n",
    "                \"total_chunks\": len(chunks),\n",
    "                \"processed_at\": datetime.now().isoformat(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        book_documents.append(chunk)\n",
    "\n",
    "        # Print section info\n",
    "        print(f\"  Section {i + 1}: {section_title}\")\n",
    "        print(f\"    Content preview: {chunk.page_content[:100]}...\")\n",
    "\n",
    "print(f\"\\nLoaded {len(book_documents)} book summary documents\")\n",
    "\n",
    "# Combine with web documents\n",
    "all_documents = web_documents + book_documents\n",
    "\n",
    "print(f\"Total documents: {len(all_documents)}\")\n",
    "print(f\"Web articles: {len(web_documents)} chunks\")\n",
    "print(f\"Book summaries: {len(book_documents)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a00f8bf",
   "metadata": {},
   "source": [
    "Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315bfcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import SupabaseVectorStore\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "vector_store = SupabaseVectorStore.from_documents(\n",
    "    all_documents, embedding=embeddings_model, client=supabase\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a815e7e",
   "metadata": {},
   "source": [
    "Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c0dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "def promptBuilder(message):\n",
    "    results = vector_store.similarity_search(message, k=3)\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for res in results:\n",
    "        context += f\"* {res.page_content}\\n\"\n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "def extract_user_messages_from_history(history):\n",
    "    \"\"\"Extract user messages from Gradio history for context building\"\"\"\n",
    "    user_messages = []\n",
    "    \n",
    "    if not history:\n",
    "        return user_messages\n",
    "    \n",
    "    for conversation in history:\n",
    "        try:\n",
    "            # Handle dict format (type=\"messages\")\n",
    "            if isinstance(conversation, dict):\n",
    "                if conversation.get(\"role\") == \"user\":\n",
    "                    content = conversation.get(\"content\", \"\")\n",
    "                    if content:\n",
    "                        user_messages.append(content)\n",
    "            \n",
    "            # Handle tuple/list format [user_msg, ai_msg]\n",
    "            elif isinstance(conversation, (list, tuple)) and len(conversation) >= 1:\n",
    "                user_msg = conversation[0]\n",
    "                if user_msg:\n",
    "                    user_messages.append(user_msg)\n",
    "                    \n",
    "        except (IndexError, KeyError, TypeError):\n",
    "            # Skip malformed entries\n",
    "            continue\n",
    "    \n",
    "    return user_messages\n",
    "\n",
    "\n",
    "def build_conversation_history(history):\n",
    "    \"\"\"Convert Gradio history format to LangChain messages\"\"\"\n",
    "    messages = []\n",
    "\n",
    "    if not history:\n",
    "        return messages\n",
    "\n",
    "    for conversation in history:\n",
    "        try:\n",
    "            # Handle dict format (type=\"messages\")\n",
    "            if isinstance(conversation, dict):\n",
    "                role = conversation.get(\"role\", \"\")\n",
    "                content = conversation.get(\"content\", \"\")\n",
    "                \n",
    "                if role == \"user\" and content:\n",
    "                    messages.append(HumanMessage(content=content))\n",
    "                elif role == \"assistant\" and content:\n",
    "                    messages.append(AIMessage(content=content))\n",
    "\n",
    "            # Handle tuple/list format [user_msg, ai_msg]\n",
    "            elif isinstance(conversation, (list, tuple)) and len(conversation) >= 2:\n",
    "                user_msg = conversation[0]\n",
    "                ai_msg = conversation[1]\n",
    "\n",
    "                if user_msg:\n",
    "                    messages.append(HumanMessage(content=user_msg))\n",
    "                if ai_msg:\n",
    "                    messages.append(AIMessage(content=ai_msg))\n",
    "                    \n",
    "        except (IndexError, KeyError, TypeError):\n",
    "            # Skip malformed entries\n",
    "            continue\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "def echo_with_context_aware_retrieval(message, history):\n",
    "    \"\"\"Version that considers history when building context\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "\n",
    "    # Build conversation context for better retrieval\n",
    "    conversation_context = \"\"\n",
    "    if history:\n",
    "        # Get last few user messages for context\n",
    "        recent_questions = extract_user_messages_from_history(history[-3:])\n",
    "        conversation_context = \" \".join(recent_questions)\n",
    "\n",
    "    # Enhanced query for vector search\n",
    "    enhanced_query = f\"{conversation_context} {message}\".strip()\n",
    "\n",
    "    # Get context from vector store using enhanced query\n",
    "    context = promptBuilder(enhanced_query)\n",
    "\n",
    "    # Load system message\n",
    "    system_content = DocumentLoaderHelper(\"config/system_message.txt\").load_document()\n",
    "\n",
    "    enhanced_system_message = f\"\"\"{system_content}\n",
    "\n",
    "Use the context provided below to answer the question.\n",
    "You will answer always in English based in European units (metric system).\n",
    "Consider the conversation history when answering to maintain context and continuity.\n",
    "\n",
    "CONTEXT: {context}\"\"\"\n",
    "\n",
    "    # Build messages\n",
    "    messages = [SystemMessage(content=enhanced_system_message)]\n",
    "\n",
    "    # Add conversation history (limited)\n",
    "    MAX_HISTORY_TURNS = 10\n",
    "    recent_history = (\n",
    "        history[-MAX_HISTORY_TURNS:] if len(history) > MAX_HISTORY_TURNS else history\n",
    "    )\n",
    "    history_messages = build_conversation_history(recent_history)\n",
    "    messages.extend(history_messages)\n",
    "\n",
    "    # Add current user message\n",
    "    messages.append(HumanMessage(content=message))\n",
    "\n",
    "    # Debug info (optional)\n",
    "    print(f\"Enhanced query: {enhanced_query}\")\n",
    "    print(f\"Total messages: {len(messages)}\")\n",
    "\n",
    "    # Get response\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "print(\"Starting Gradio demo...\")\n",
    "\n",
    "demo = gradio.ChatInterface(\n",
    "    fn=echo_with_context_aware_retrieval,\n",
    "    type=\"messages\",\n",
    "    examples=[\n",
    "        \"How can I improve my sleep?\",\n",
    "        \"How can I improve my VO2 max?\",\n",
    "        \"What consist the zone 2 training approach?\",\n",
    "    ],\n",
    "    title=\"Longevity AI\",\n",
    ")\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af974b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
